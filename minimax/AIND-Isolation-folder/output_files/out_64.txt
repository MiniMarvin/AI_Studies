
This script evaluates the performance of the custom_score evaluation
function against a baseline agent using alpha-beta search and iterative
deepening (ID) called `AB_Improved`. The three `AB_Custom` agents use
ID and alpha-beta search with the custom_score functions defined in
game_agent.py.

                        *************************                         
                             Playing Matches                              
                        *************************                         

 Match #   Opponent    AB_Improved   AB_Custom   AB_Custom_2  AB_Custom_3 
                        Won | Lost   Won | Lost   Won | Lost   Won | Lost 
    1       Random       9  |   1     9  |   1     8  |   2    10  |   0  
    2       MM_Open      5  |   5     6  |   4     6  |   4     7  |   3  
    3      MM_Center     7  |   3     8  |   2     6  |   4     7  |   3  
    4     MM_Improved    6  |   4     6  |   4     4  |   6     6  |   4  
    5       AB_Open      4  |   6     4  |   6     3  |   7     4  |   6  
    6      AB_Center     6  |   4     4  |   6     6  |   4     4  |   6  
    7     AB_Improved    6  |   4     5  |   5     6  |   4     5  |   5  
--------------------------------------------------------------------------
           Win Rate:      61.4%        60.0%        55.7%        61.4%    

There were 1.0 timeouts during the tournament -- make sure your agent handles search timeout correctly, and consider increasing the timeout margin for your agent.


Your agents forfeited 169.0 games while there were still legal moves available to play.

